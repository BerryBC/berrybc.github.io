---
layout: post
title:  廿-爬URL以及分词情绪分析初步设想
date:   2020.02.01 17:45:49 +0800
categories: 三世书记录
author: 崔秉龙
location: Guangzhou, China
---



# 1.爬网络页面链接

其实是想捉取大量 `网络内容` 以对以后建立 `情绪` 爬虫作为 `样本` ，设想就是不断把所有网络 `可读取` 的 `页面` 都抓下来。

- ## 1.1 数据库设计

以一个 `数据库` 下设两个 `表` 保存

```
dbPage
  |
  |-- tbReusablePage
  └-- tbCrawledPage
```

其中表格 `字段` 设置如下：

```js
tbCrawledPage:
{
  'url': {string} with index //页面URL
  'd':{integer} d for depth //页面深度，即是有多少个 '/'
  'ced':{boolean} for crawled //是否已经抓取
  'jed':{boolean} for judged //是否已经判断
  't':{date} for time //何时存入
}

tbReusablePage:
{
  'url': {string} with index //页面URL
}
```

- ## 1.2 思路

每隔一段时间从 `重用页面库 (tbReusablePage)` 爬一次，把捉取结果分析出对象页面所有的 `<a>` 的 `指向页面`，并保存到 `已捉取页面库 (tbCrawledPage) `。

- ## 1.3 伪代码
- #### 主程序

爬取重用页面()

爬已爬取页面()

定期清除过期的已爬页面()


- #### 爬取重用页面

对 重用页面库 所有页面进行捉取

设定时任务 ( 爬取重用页面() )


- #### 爬已爬取页面

从 已捉取页面库 中随机找出数十个未爬的页面

更新这些页面在 已捉取页面库 的状态

捉着这些页面所有 `<a>` 的链接

放入数据库()

设定时任务 (爬已爬取页面() )


- #### 定期清除过期的已爬页面

清除超过一定深度以及放入时间距今超过数个月的页面

设定时任务 (定期清除过期的已爬页面() )


- ## 1.4 小结

此程序仅为不断 `自我调用` 的捉取网站的页面链接并 `保存`，仅供以后判断时做 `样本` 用。

# 2.情绪分析助手

- ## 2.1 一个解释

暂时还没决定如何写，先看看通过哪些来实现，然后看看这些 `包` 能做什么和怎么做，输出结果如何再想下一步。

- ## 2.2 分段
1.  `结巴分词` Python版试用
2. `Django` 做页面
2.1 判断页面 `价值、情绪`，对页面是否有用，以及情绪是 `正面、中立、负面`
2.2 重用页面 `管理`，对 `重用页面` 的增减进行管理
2.3 数据库信息查看以及 `Log`
3. `机器学习` 试用
看 Python 的 `机器学习` 有哪些 `包`，输入输出结果如何，有以下这几种：
`聚类`、`神经网络`、`决策树`、`贝叶斯`。
（因为上述是原理我看得懂，而且比较 `常见` 的算法）
4. 半监督生成
先通过判定某些词是正面词，某些词是负面词，然后再把预测结果当作学习样本， `重新生成` 新的决策方式。
这样需要做一个 `版本存储`，以备新生成的决策方式是一个差的决策树。

- ## 2.3 初步数据库设定

比较简单的 `样本存储` 数据库设定如下：

```
dbSample
  |
  └-- tbSample
```

其中 tbSample 字段设置如下：

```js
tbSample:
{
  'ct': {text} ct for content //样本内容
  'e':{integer} e for emotion //情绪是什么，从-1、0、1，理论上0为不作处理，即无价值
  'cf':{boolean} for confirm //是否经人工确认
}
```